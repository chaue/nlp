{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with natural language processing tasks such as translation or prediction, the goal is often to find a model that is the most uniform while still fulfilling preset constraints based on prior knowledge/data. By considering models that are uniformly distributed outside of constraints, the hope is to most accurately describe stochastic processes present in data sets by picking the most \"general\" model. \n",
    "\n",
    "For example, if we know the word **water** is always followed the word **bottle**, we can set that tuple as a constraint and have all other words in our data set be equally likely to be followed by any other word.\n",
    "\n",
    "This maximum entropy function with respect to a conditional probability $p(y|x)$ is given by\n",
    "\n",
    "$$ H(p) = -\\sum_{x,y} \\tilde{p}(x)p(y|x)\\log p(y|x) $$\n",
    "\n",
    "However, maximizing $H(p)$ also has the constraints \n",
    "\n",
    "$$ p(f_i) = \\tilde{p}(f_i) $$\n",
    "\n",
    "where $f$ is an indicator function with respect to a tuple $(x,y)$, $p(f_i)$ is the expected value of $f$ with respect to the empirical distribution of a tuple $(x,y)$, and $\\tilde{p}(f_i)$ is the expected value of $f$ with respect to $p(y|x)$ . We will discuss these and similar functions and their definitions in further detail when implementing this method.\n",
    "\n",
    "To avoid dealing with these constraints when finding the optimum, we instead minimize the dual of $H(p)$. This dual function is given by\n",
    "\n",
    "$$ \\psi(\\lambda) = - \\sum_{x} \\tilde{p}(x) \\log{(\\sum_{y} \\exp{(\\sum_{i} \\lambda_i f_i (x,y))})} + \\sum_{i} \\lambda_i \\tilde{p}(f_i)$$\n",
    "\n",
    "Because our primal function $H(p)$ is symmetric, our dual is therefore unbounded, that is $\\lambda \\in \\rm I\\!R$. This dual function should be relatively easier to minimize compared to our original primal problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "\n",
    "To start, we import the NLTK package, which contains useful functions as well as formatted text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from __future__ import division\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define intermediate functions that will combine to form the dual function.\n",
    "\n",
    "$\\tilde{p}(x)$ is the empirical distribution of a word $x$, and so we count the number of occurences of $x$ and divide it by the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p~(x): count occurences of word x\n",
    "def ptildex(text, x):\n",
    "    return(txt.count(x) / len(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_i(x,y)$ is an indicator (and in our case, a feature) function corresponding to a tuple of words (x,y). We will have it return $1$ if the input **tup** matches a specific tuple **z** we chose as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(tup, z):\n",
    "    if tup == z: return(1)\n",
    "    else: return(0)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{p}(f)$, as mentioned earlier, is the expected value of a feature function $f(x,y)$ with respect to $\\tilde{p}(x,y)$, the empirical distribution of $(x,y)$. Specifically, it is defined as \n",
    "\n",
    "$$ \\tilde{p}(f) = \\sum_{x,y}\\tilde{p}(x,y) f(x,y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p~(x,y): count occurences of a pair (x, y)\n",
    "def ptildexy(text, tup):\n",
    "    bg = list(bigrams(text))\n",
    "    return(bg.count(tup) / len(bg))\n",
    "\n",
    "def ptildef(bigramset, f):\n",
    "    val = 0\n",
    "    for pair in bigramset:\n",
    "        val += ptildexy(txt, pair)*float(f(pair, ('hey', 'no')))\n",
    "        # the feature tuple for f is filled in for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an important NLTK function we will be using is the bigrams function, which accepts a text file input and returns a list of all sequential bigrams. \n",
    "\n",
    "For example, bigrams([yes, no, maybe]) will return (yes, no), (no, maybe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hey', 'no'), ('no', 'ucla'), ('ucla', 'math'), ('math', 'derivative'), ('derivative', 'bus'), ('bus', 'computer'), ('computer', 'hello')]\n",
      "0.125\n",
      "0.142857142857\n",
      "0\n",
      "0.142857142857\n"
     ]
    }
   ],
   "source": [
    "## bigrams function example\n",
    "txt = ['hey', 'no', 'ucla', 'math', 'derivative', 'bus', 'computer', 'hello']\n",
    "bg = list(bigrams(txt))\n",
    "print(bg)\n",
    "\n",
    "## p~(x): count occurences of word x\n",
    "def ptildex(text, x):\n",
    "    return(txt.count(x) / len(txt))\n",
    "print(ptildex(txt, 'ucla'))\n",
    "\n",
    "## p~(x,y): count occurences of a pair (x, y)\n",
    "def ptildexy(text, tup):\n",
    "    bg = list(bigrams(text))\n",
    "    return(bg.count(tup) / len(bg))\n",
    "print(ptildexy(txt, ('no', 'ucla')))\n",
    "\n",
    "#print(bg.count(('hey', 'no')))\n",
    "\n",
    "## f(x,y) feature function, z is an element of a set of features (x,y)\n",
    "def f(tup, z):\n",
    "    if tup == z: return(1)\n",
    "    else: return(0)     \n",
    "print(f(bg[1], ('bus', 'computer')))\n",
    "\n",
    "def ptildef(bigramset, f):\n",
    "    val = 0\n",
    "    for pair in bigramset:\n",
    "        val += ptildexy(txt, pair)*float(f(pair, ('hey', 'no')))\n",
    "    print(val)\n",
    "ptildef(bg, f)    \n",
    "\n",
    "\n",
    "lambdaf = 0\n",
    "for l, f in zip(lam, feat):\n",
    "    lambdaf += l*f((x,y))\n",
    "    \n",
    "## freq dist example\n",
    "#txt2 = nltk.tokenize.word_tokenize(txt)\n",
    "#dist = FreqDist(txt2)\n",
    "#print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
