{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from __future__ import division, mul\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.count('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hey', 'no'), ('no', 'ucla'), ('ucla', 'math'), ('math', 'derivative'), ('derivative', 'bus'), ('bus', 'computer'), ('computer', 'hello')]\n",
      "0.125\n",
      "0.142857142857\n",
      "0\n",
      "0.142857142857\n"
     ]
    }
   ],
   "source": [
    "## bigrams function example\n",
    "txt = ['hey', 'no', 'ucla', 'math', 'derivative', 'bus', 'computer', 'hello']\n",
    "bg = list(bigrams(txt))\n",
    "print(bg)\n",
    "\n",
    "## p~(x): count occurences of word x\n",
    "def ptildex(text, x):\n",
    "    return(txt.count(x) / len(txt))\n",
    "print(ptildex(txt, 'ucla'))\n",
    "\n",
    "## p~(x,y): count occurences of a pair (x, y)\n",
    "def ptildexy(text, tup):\n",
    "    bg = list(bigrams(text))\n",
    "    return(bg.count(tup) / len(bg))\n",
    "print(ptildexy(txt, ('no', 'ucla')))\n",
    "#print(bg.count(('hey', 'no')))\n",
    "\n",
    "## f(x,y) feature function, z is an element of a set of features (x,y)\n",
    "def f(tup, z):\n",
    "    if tup == z: return(1)\n",
    "    else: return(0)     \n",
    "print(f(bg[1], ('bus', 'computer')))\n",
    "\n",
    "def ptildef(bigramset, f):\n",
    "    val = 0\n",
    "    for pair in bigramset:\n",
    "        val += ptildexy(txt, pair)*float(f(pair, ('hey', 'no')))\n",
    "    print(val)\n",
    "ptildef(bg, f)    \n",
    "\n",
    "\n",
    "lambdaf = 0\n",
    "for l, f in zip(lam, feat):\n",
    "    lambdaf += l*f((x,y))\n",
    "    \n",
    "## freq dist example\n",
    "#txt2 = nltk.tokenize.word_tokenize(txt)\n",
    "#dist = FreqDist(txt2)\n",
    "#print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with natural language processing tasks such as translation or prediction, the goal is often to find a model that is the most uniform while still fulfilling preset constraints based on prior knowledge/data. By considering models that are uniformly distributed outside of constraints, the hope is to most accurately describe stochastic processes present in data sets by picking the most \"general\" model. \n",
    "\n",
    "For example, if we know the word **water** is always followed the word **bottle**, we can set that pairing as a constraint and have all other words in our data set be equally likely to be followed by any other word.\n",
    "\n",
    "This maximum entropy function with respect to a conditional probability $p(y|x)$ is given by\n",
    "\n",
    "$$ H(p) = -\\sum_{x,y} \\tilde{p}(x)p(y|x)\\log p(y|x) $$\n",
    "\n",
    "However, maximizing $H(p)$ also has the constraints that your empirical distributions of words $x_i$ is equal to their expected value, or the number of times $x_i$ should appear in your dataset. That is,\n",
    "\n",
    "$$ p(f) = \\tilde{p}(f) $$\n",
    "\n",
    "where $f$ is an indicator function equaling 1 if a pair is $(x,y)$ and 0 otherwise.\n",
    "\n",
    "To avoid these constraints when finding the optimum, we instead minimize the dual of $H(p)$, which is our primal function. This dual function is given by\n",
    "\n",
    "$$ \\psi(\\lambda) = - \\sum_{x} \\tilde{p}(x) \\log{(\\sum_{y} \\exp{(\\sum_{i} \\lambda_i f_i (x,y))})} + \\sum_{i} \\lambda_i \\tilde{p}(f_i)$$\n",
    "\n",
    "Because $H(p)$ is symmetric, our dual is therefore unbounded, that is $\\lambda \\in \\rm I\\!R$. This dual function should be relatively easier to minimize compared to our original primal problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
