{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with natural language processing tasks such as translation or prediction, the goal is often to find a model that is the most uniform while still fulfilling preset constraints based on prior knowledge/data. By considering models that are uniformly distributed outside of constraints, the hope is to most accurately describe stochastic processes present in data sets by picking the most \"general\" model. \n",
    "\n",
    "For example, if we know the word **water** is always followed the word **bottle**, we can set that tuple as a constraint and have all other words in our data set be equally likely to be followed by any other word.\n",
    "\n",
    "This maximum entropy function with respect to a conditional probability $p(y|x)$ is given by\n",
    "\n",
    "$$ H(p) = -\\sum_{x,y} \\tilde{p}(x)p(y|x)\\log p(y|x) $$\n",
    "\n",
    "However, maximizing $H(p)$ also has the constraints \n",
    "\n",
    "$$ p(f_i) = \\tilde{p}(f_i) $$\n",
    "\n",
    "where $f$ is an indicator function with respect to a tuple $(x,y)$, $p(f_i)$ is the expected value of $f$ with respect to the empirical distribution of a tuple $(x,y)$, and $\\tilde{p}(f_i)$ is the expected value of $f$ with respect to $p(y|x)$ . We will discuss these and similar functions and their definitions in further detail when implementing this method.\n",
    "\n",
    "To avoid dealing with these constraints when finding the optimum, we instead minimize the dual of $H(p)$. This dual function is given by\n",
    "\n",
    "$$ \\psi(\\lambda) = - \\sum_{x} \\tilde{p}(x) \\log{(\\sum_{y} \\exp{(\\sum_{i} \\lambda_i f_i (x,y))})} + \\sum_{i} \\lambda_i \\tilde{p}(f_i)$$\n",
    "\n",
    "Because our primal function $H(p)$ is symmetric, our dual is therefore unbounded, that is $\\lambda \\in \\rm I\\!R$. This dual function should be relatively easier to minimize compared to our original primal problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "\n",
    "To start, we import the NLTK package, which contains useful functions as well as formatted text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize \n",
    "from __future__ import division\n",
    "\n",
    "# only download once\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define intermediate functions that will combine to form the dual function.\n",
    "\n",
    "$\\tilde{p}(x)$ is the empirical distribution of a word $x$, and so we count the number of occurences of $x$ and divide it by the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p~(x): count occurences of word x\n",
    "def ptildex(text, x):\n",
    "    return(txt.count(x) / len(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_i(x,y)$ is an indicator (and in our case, a feature) function corresponding to a tuple of words (x,y). We will have it return $1$ if the input **tup** matches a specific tuple **z** we chose as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(tup, z):\n",
    "    if tup == z: return(1)\n",
    "    else: return(0)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{p}(f)$, as mentioned earlier, is the expected value of a feature function $f(x,y)$ with respect to $\\tilde{p}(x,y)$, the empirical distribution of $(x,y)$. Specifically, it is defined as \n",
    "\n",
    "$$ \\tilde{p}(f) = \\sum_{x,y}\\tilde{p}(x,y) f(x,y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p~(x,y): count occurences of a pair (x, y)\n",
    "def ptildexy(text, tup):\n",
    "    bg = list(nltk.bigrams(text))\n",
    "    return(bg.count(tup) / len(bg))\n",
    "\n",
    "def ptildef(text, bigramset, feat):\n",
    "    val = float(0)\n",
    "    for pair in bigramset:\n",
    "        val += ptildexy(text, pair)*float(f(pair, feat))\n",
    "    return(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would also be convenient to define \n",
    "$$\\log{(\\sum_{y} \\exp{(\\sum_{i} \\lambda_i f_i (x,y))})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsum(x, lambdas, features):\n",
    "    outersum = float(0)\n",
    "    innersum = float(0)\n",
    "    for y in sety:\n",
    "        for lam, feat in zip(lambdas, features):\n",
    "            innersum += lam * f((x,y), feat)\n",
    "        outersum += np.exp(innersum)\n",
    "    return(np.log(outersum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an important NLTK function we will be using is the bigrams function, which accepts a text file input and returns a list of all sequential word pairs. For example, bigrams(yes, no, maybe) will return (yes, no), (no, maybe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'no', 'ucla', 'math', 'derivative', 'bus', 'computer', 'hello']\n",
      "[('hey', 'no'), ('no', 'ucla'), ('ucla', 'math'), ('math', 'derivative'), ('derivative', 'bus'), ('bus', 'computer'), ('computer', 'hello')]\n"
     ]
    }
   ],
   "source": [
    "txt = ['hey', 'no', 'ucla', 'math', 'derivative', 'bus', 'computer', 'hello']\n",
    "bg = list(nltk.bigrams(txt))\n",
    "print(txt)\n",
    "print(bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can piece together the dual function. It takes a vector of initial lambda values and sums over all $x$ and $y$, which are the sets of unique words in our text file. In our case x and y are equal, as every word can either be predicted or predicted from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['test']\n",
    "setx = set(text)\n",
    "sety = set(text)\n",
    "bg = list(nltk.bigrams(text))\n",
    "\n",
    "def dual(lambdas):\n",
    "    firstsum = float(0)\n",
    "    for x in setx:\n",
    "        firstsum -= ptildex(text, x) * logsum(x, lambdas, features)\n",
    "    secondsum = float(0)\n",
    "    for lam, feat in zip(lambdas, features):\n",
    "        secondsum += lam * ptildef(text, bigramset, feat)\n",
    "    return (firstsum + secondsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a Quasi-Newton method we need to also provide a gradient, which we can achieve using simple calculus\n",
    "$$ \\dfrac{\\partial \\psi(\\lambda)}{\\partial{\\lambda_i}} = -\\sum_{x}\\tilde{p}(x) \\frac{\\sum_{y}(\\exp{(\\sum_{i} \\lambda_i f_i(x,y))}f_i (x,y))}{\\sum_{y} \\exp{(\\sum_{i} \\lambda_i f_i (x,y))}} + \\tilde{p}(f_i)$$\n",
    "\n",
    "since \n",
    "\n",
    "$$ \\dfrac{\\partial}{\\partial \\lambda_i} \\sum_{i} \\lambda_i f_i(x,y) = f_i(x,y) $$\n",
    "\n",
    "and where $ \\dfrac{\\partial \\psi(\\lambda)}{\\partial{\\lambda_i}}$ is the ith component of our gradient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expterm(x, y, lambdas, features):\n",
    "    innersum = float(0)\n",
    "    for lam, feat in zip(lambdas, features):\n",
    "        innersum += lam * f((x,y), feat)\n",
    "    return(np.exp(innersum))\n",
    "\n",
    "def grad(lambdas):\n",
    "    firstsum = 0\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    gradient = np.empty((1,1))\n",
    "    for i in np.arange(0, len(lambdas)):\n",
    "        for x in setx:\n",
    "            for y in sety:\n",
    "                numerator += expterm(x, y, lambdas, features)\n",
    "            denominator = numerator\n",
    "            numerator = numerator*f((x, y), features[i])\n",
    "            firstsum -= (numerator/denominator) * ptildex(text, x)\n",
    "        firstsum += ptildef(text, bigramset, features[i])\n",
    "        gradient = np.append(gradient, firstsum)\n",
    "    gradient = gradient[1:]\n",
    "    return(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test functions for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8400000000000001\n",
      "[0.16 0.2 ]\n"
     ]
    }
   ],
   "source": [
    "text = ['In', 'optimization', 'quasi-Newton', 'methods', 'are', 'algorithms', 'for', 'finding', 'local', \n",
    "        'maxima', 'and', 'minima', 'of', 'functions', 'In', 'quasi-Newton', 'methods', 'the', 'Hessian',\n",
    "        'matrix', 'does', 'not', 'need', 'to', 'be', 'computed']\n",
    "bigramset = list(nltk.bigrams(text))\n",
    "setx = set(text)\n",
    "sety = set(text)\n",
    "features = [('quasi-Newton', 'methods'), ('local', 'maxima')]\n",
    "lambdas = [4, 5]\n",
    "\n",
    "print(dual(lambdas))\n",
    "print(grad(lambdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edwin\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:1013: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (numpy.dot(yk, sk))\n",
      "C:\\Users\\Edwin\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:1025: RuntimeWarning: overflow encountered in multiply\n",
      "  sk[numpy.newaxis, :])\n",
      "C:\\Users\\Edwin\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:273: RuntimeWarning: invalid value encountered in multiply\n",
      "  return f(xk + alpha * pk, *args)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: -6.185681452892673e+152\n",
       " hess_inv: array([[inf, inf],\n",
       "       [inf, inf]])\n",
       "      jac: array([0.16, 0.2 ])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 3863\n",
       "      nit: 51\n",
       "     njev: 3862\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([-2.94556260e+153, -3.68195325e+153])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(dual, lambdas, method='BFGS', jac=grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular and computationally cheaper optimization method is a gradient method, which iteratively steps in a direction that minimizes the function(usually determined by the gradient). First we set $\\lambda = 0$. Then for each $\\lambda_i$ we increment it by $\\Delta \\lambda_i$ so that $\\lambda_i = \\lambda_i + \\Delta \\lambda_i$ where $\\Delta \\lambda_i$ is the solution to \n",
    "\n",
    "$$ \\sum_{x,y} \\tilde{p}(x) p(y|x) f_i(x,y) \\exp{(\\Delta \\lambda_i \\sum_{i} f_i(x,y))} = \\tilde{p}(f_i)$$\n",
    "\n",
    "We then check for convergence of $\\lambda_i$ and repeat the increment of $\\lambda_i$ if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
