{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with natural language processing tasks such as translation or prediction, the goal is often to find a model that is the most uniform while still fulfilling preset constraints based on prior knowledge/data. By considering models that are uniformly distributed outside of constraints, the hope is to most accurately describe stochastic processes present in data sets by picking the most \"general\" model. \n",
    "\n",
    "For example, if we know the word **water** is always followed the word **bottle**, we can set that tuple as a constraint and have all other words in our data set be equally likely to be followed by any other word.\n",
    "\n",
    "This maximum entropy function with respect to a conditional probability $p(y|x)$ is given by\n",
    "\n",
    "$$ H(p) = -\\sum_{x,y} \\tilde{p}(x)p(y|x)\\log p(y|x) $$\n",
    "\n",
    "However, maximizing $H(p)$ also has the constraints \n",
    "\n",
    "$$ p(f_i) = \\tilde{p}(f_i) $$\n",
    "\n",
    "where $f$ is an indicator function with respect to a tuple $(x,y)$, $p(f_i)$ is the expected value of $f$ with respect to the empirical distribution of a tuple $(x,y)$, and $\\tilde{p}(f_i)$ is the expected value of $f$ with respect to $p(y|x)$ . We will discuss these and similar functions and their definitions in further detail when implementing this method.\n",
    "\n",
    "To avoid dealing with these constraints when finding the optimum, we instead minimize the dual of $H(p)$. This dual function is given by\n",
    "\n",
    "$$ \\psi(\\lambda) = - \\sum_{x} \\tilde{p}(x) \\log{(\\sum_{y} \\exp{(\\sum_{i} \\lambda_i f_i (x,y))})} + \\sum_{i} \\lambda_i \\tilde{p}(f_i)$$\n",
    "\n",
    "Because our primal function $H(p)$ is symmetric, our dual is therefore unbounded, that is $\\lambda \\in \\rm I\\!R$. This dual function should be relatively easier to minimize compared to our original primal problem. \n",
    "\n",
    "After calculating our dual variables $\\lambda$ we aim to calculate \n",
    "\n",
    "$$ p_\\lambda (y|x) = \\dfrac{\\exp{(\\sum_{i}\\lambda_i f_i(x,y))}}{\\sum_{y} \\exp{(\\sum_{i} \\lambda_i f_i (x,y))}} $$\n",
    "\n",
    "which will give the prediction $y$ given a word $x$ based on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "\n",
    "To start, we import the NLTK package, which contains useful functions as well as formatted text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 9 ms\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import scipy.optimize \n",
    "from __future__ import division\n",
    "\n",
    "%load_ext autotime\n",
    "\n",
    "# only download once\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define intermediate functions that will combine to form the dual function.\n",
    "\n",
    "$\\tilde{p}(x)$ is the empirical distribution of a word $x$, and so we count the number of occurences of $x$ and divide it by the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "## p~(x): count occurences of word x\n",
    "def ptildex(text, x):\n",
    "    return(text.count(x) / len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_i(x,y)$ is an indicator (and in our case, a feature) function corresponding to a tuple of words (x,y). We will have it return $1$ if the input **tup** matches a specific tuple **z** we chose as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "def f(tup, z):\n",
    "    if tup == z: return(1)\n",
    "    else: return(0)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{p}(f)$, as mentioned earlier, is the expected value of a feature function $f(x,y)$ with respect to $\\tilde{p}(x,y)$, the empirical distribution of $(x,y)$. Specifically, it is defined as \n",
    "\n",
    "$$ \\tilde{p}(f) = \\sum_{x,y}\\tilde{p}(x,y) f(x,y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7 ms\n"
     ]
    }
   ],
   "source": [
    "## p~(x,y): count occurences of a pair (x, y)\n",
    "def ptildexy(text, tup):\n",
    "    bg = list(nltk.bigrams(text))\n",
    "    return(bg.count(tup) / len(bg))\n",
    "\n",
    "def ptildef(text, bigramset, feat):\n",
    "    val = float(0)\n",
    "    for pair in bigramset:\n",
    "        val += ptildexy(text, pair)*float(f(pair, feat))\n",
    "    return(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would also be convenient to define \n",
    "$$\\log{(\\sum_{y} \\exp{(\\sum_{i} \\lambda_i f_i (x,y))})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9 ms\n"
     ]
    }
   ],
   "source": [
    "def expterm(x, y, lambdas, features):\n",
    "    total = 0\n",
    "    for lam, feat in zip(lambdas, features):\n",
    "        total += lam * f((x,y), feat)\n",
    "    return(np.exp(total))\n",
    "\n",
    "def logsum(x, lambdas, features):\n",
    "    outersum = 0\n",
    "    for y in sety:\n",
    "        outersum += expterm(x, y, lambdas, features)\n",
    "    return(np.log(outersum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an important NLTK function we will be using is the bigrams function, which accepts a text file input and returns a list of all sequential word pairs. For example, bigrams(yes, no, maybe) will return (yes, no), (no, maybe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'no', 'ucla', 'math', 'derivative', 'bus', 'computer', 'hello']\n",
      "[('hey', 'no'), ('no', 'ucla'), ('ucla', 'math'), ('math', 'derivative'), ('derivative', 'bus'), ('bus', 'computer'), ('computer', 'hello')]\n",
      "time: 7 ms\n"
     ]
    }
   ],
   "source": [
    "txt = ['hey', 'no', 'ucla', 'math', 'derivative', 'bus', 'computer', 'hello']\n",
    "bg = list(nltk.bigrams(txt))\n",
    "print(txt)\n",
    "print(bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can piece together the dual function. It takes a vector of initial lambda values and sums over all $x$ and $y$, which are the sets of unique words in our text file. In our case x and y are equal, as every word can either be predicted or predicted from. \n",
    "\n",
    "An important thing to note is that this implementation returns a function value of the opposite sign, as we aim to maximize the dual(and therefore minimize the negative of the dual). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "def negativedual(lambdas):\n",
    "    firstsum = float(0)\n",
    "    for x in setx:\n",
    "        firstsum -= ptildex(text, x) * logsum(x, lambdas, features)\n",
    "    secondsum = float(0)\n",
    "    for lam, feat in zip(lambdas, features):\n",
    "        secondsum += lam * ptildef(text, bigramset, feat)\n",
    "    return ((-1)*(firstsum + secondsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a Quasi-Newton method we need to also provide a gradient, which we can achieve using simple calculus\n",
    "$$ \\dfrac{\\partial \\psi(\\lambda)}{\\partial{\\lambda_i}} = -\\sum_{x}\\tilde{p}(x) \\frac{\\sum_{y}(\\exp{(\\sum_{i} \\lambda_i f_i(x,y))}f_i (x,y))}{\\sum_{y} \\exp{(\\sum_{i} \\lambda_i f_i (x,y))}} + \\tilde{p}(f_i)$$\n",
    "\n",
    "since \n",
    "\n",
    "$$ \\dfrac{\\partial}{\\partial \\lambda_i} \\sum_{i} \\lambda_i f_i(x,y) = f_i(x,y) $$\n",
    "\n",
    "and where $ \\dfrac{\\partial \\psi(\\lambda)}{\\partial{\\lambda_i}}$ is the ith component of our gradient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "def negativegrad(lambdas):\n",
    "    firstsum = 0\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    gradient = np.empty((1,1))\n",
    "    for i in np.arange(0, len(lambdas)):\n",
    "        for x in setx:\n",
    "            for y in sety:\n",
    "                denominator += expterm(x, y, lambdas, features)\n",
    "                numerator += denominator * f((x,y), features[i])\n",
    "            firstsum -= (numerator/denominator) * ptildex(text, x)\n",
    "        firstsum += ptildef(text, bigramset, features[i])\n",
    "        gradient = np.append(gradient, -1*firstsum)\n",
    "    gradient = gradient[1:]\n",
    "    return(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal model given an array of text and features, we initialize $\\lambda$ as a vector of values and use the minimize function in the scipy.optimize library. $\\lambda$ has an equal number of entries as there are features, as the number of dual variables matches the number of constraints in our primal problem.\n",
    "\n",
    "## Super Simple Example\n",
    "\n",
    "Let's start with a simple example of just 4 words and 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 1.3862943611198906\n",
      " hess_inv: array([[1, 0],\n",
      "       [0, 1]])\n",
      "      jac: array([0.03125   , 0.88764881])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 107\n",
      "      nit: 0\n",
      "     njev: 95\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([0., 0.])\n",
      "time: 52 ms\n"
     ]
    }
   ],
   "source": [
    "text = ['optimization', 'is', 'very', 'complex']\n",
    "bigramset = list(nltk.bigrams(text))\n",
    "setx = set(text)\n",
    "sety = set(text)\n",
    "features = [('optimization', 'is'), ('very', 'complex')]\n",
    "lambdas = [0, 0]\n",
    "\n",
    "result = scipy.optimize.minimize(negativedual, lambdas, method='BFGS', jac=negativegrad)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the BFGS algorithm fails even in this simple case. It is unclear why, but the output indicates that a more optimal value was not able to be found. This is obviously contradictory to what we'd expect, as the gradient is nonzero at our initial point. A reason for this issue could be that the direction the algorithm is attempting to perturb in is very tiny, and the round off error causes it to assume there are no directions that lead to a smaller function value.\n",
    "\n",
    "We can also attempt to optimize using the Nelder-Mead method, which does not require the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " final_simplex: (array([[707.88495367, 709.78270615],\n",
      "       [707.88488304, 709.78263525],\n",
      "       [707.88487116, 709.78262333]]), array([-117.44582447, -117.44581268, -117.44581069]))\n",
      "           fun: -117.44582447101908\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 174\n",
      "           nit: 91\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([707.88495367, 709.78270615])\n",
      "time: 36 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edwin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "result = scipy.optimize.minimize(negativedual, lambdas, method='nelder-mead')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! The function output indicates that the algorithm finished 94 interations before converging to our minimum. The dual variables are given by the returned vector x, and we can use these to make predictions using $p_\\lambda (y|x)$. First we need to implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "def ygivenx(x, y, lambdas, features):\n",
    "    numerator = expterm(x, y, lambdas, features)\n",
    "    denominator = 0\n",
    "    for yi in sety:\n",
    "        denominator += expterm(x, yi, lambdas, features)\n",
    "    return(numerator/denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the solution in our optimization results, we plug it into $p_\\lambda (y|x)$ along with words $x$ and $y$. The model will then return a value between 0 and 1, which represents the probability of predicting $y$ given the input $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very -> very :  0.0\n",
      "very -> is :  0.0\n",
      "very -> optimization :  0.0\n",
      "very -> complex :  1.0\n",
      "is -> very :  0.25\n",
      "is -> is :  0.25\n",
      "is -> optimization :  0.25\n",
      "is -> complex :  0.25\n",
      "optimization -> very :  0.0\n",
      "optimization -> is :  1.0\n",
      "optimization -> optimization :  0.0\n",
      "optimization -> complex :  0.0\n",
      "complex -> very :  0.25\n",
      "complex -> is :  0.25\n",
      "complex -> optimization :  0.25\n",
      "complex -> complex :  0.25\n",
      "time: 27 ms\n"
     ]
    }
   ],
   "source": [
    "xs = set(text)\n",
    "ys = set(text)\n",
    "\n",
    "for x in xs:\n",
    "    for y in ys:\n",
    "        print x, \"->\", y, \": \", np.round(ygivenx(x, y, result.x, features), decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results of our model, it is (thankfully) consistent with what we expect from it. \n",
    "\n",
    "Because our two features required 'is' to be the prediction for 'optimization' and 'complex' to be the prediction for 'very', the model ensured these predictions will always happen(as evident by the probability of 1). All other predictions in these cases are set to 0, as there should be no other options.\n",
    "\n",
    "In cases where the model was given a word that was not part of a feature, in this case the words 'is' and 'complex', all predictions are uniform, which is consistent with the maximum entropy nature of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harder Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our text lengths will get too long to show the probabilities for all permutations, we can write a function that only reports the maximum probability and word associated with it, which in our case would be the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7 ms\n"
     ]
    }
   ],
   "source": [
    "def maxprob(x, sety, dualsol, features):\n",
    "    maxval = 0\n",
    "    prediction = \"\"\n",
    "    for y in sety:\n",
    "        #print(ygivenx(x, y, dualsol, features))\n",
    "        if ygivenx(x, y, dualsol, features) > maxval:\n",
    "            maxval = ygivenx(x, y, dualsol, features)\n",
    "            prediction = y\n",
    "    print x, \"->\", y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edwin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n",
      "C:\\Users\\Edwin\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:663: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  grad[k] = (f(*((xk + d,) + args)) - f0) / d[k]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 3.3552011206846117\n",
      " hess_inv: array([[46373.96592142, 12839.59174587],\n",
      "       [12839.59174587,  3556.02393698]])\n",
      "      jac: array([-0.00156865, -0.00046   ])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 102\n",
      "      nit: 6\n",
      "     njev: 23\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([34.65549051,  9.42457665])\n",
      "time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "text = ['In', 'optimization', 'quasi-Newton', 'methods', 'are', 'algorithms', 'for', 'finding', 'local', \n",
    "        'maxima', 'and', 'minima', 'of', 'functions', 'In', 'quasi-Newton', 'methods', 'the', 'Hessian',\n",
    "        'matrix', 'does', 'not', 'need', 'to', 'be', 'computed', 'The', 'first', 'quasi-Newton', 'method',\n",
    "        'was', 'proposed', 'by', 'William', 'C', 'Davidon', 'a', 'physicist', 'working', 'at', 'Argonne', \n",
    "        'National', 'Laboratory', 'He', 'developed', 'the', 'first', 'quasi-Newton', 'algorithm', 'in', '1959']\n",
    "bigramset = list(nltk.bigrams(text))\n",
    "setx = set(text)\n",
    "sety = set(text)\n",
    "features = [('quasi-Newton', 'methods'), ('local', 'maxima')]\n",
    "lambdas = [0, 0]\n",
    "\n",
    "result = scipy.optimize.minimize(negativedual, lambdas, method='BFGS')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quasi-Newton -> methods:  1.0\n",
      "local -> maxima:  0.997\n",
      "methods -> maxima\n",
      "Quasi-Newton -> maxima\n",
      "time: 34 ms\n"
     ]
    }
   ],
   "source": [
    "print \"quasi-Newton -> methods: \", np.round(ygivenx('quasi-Newton', 'methods', result.x, features), decimals=3)\n",
    "print \"local -> maxima: \", np.round(ygivenx('local', 'maxima', result.x, features), decimals=3)\n",
    "maxprob('methods', set(text), result.x, features)\n",
    "maxprob('Quasi-Newton', set(text), result.x, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the predictions are consistent with our features. One important thing to note is even though we have a maximum probability function, the uniformity of our model forces the output to be arbitrary if the given word is not part of a feature. In our example, the maxprob function tells us 'maxima' has the highest probability of being predicted, but in reality all words have equal probability. \n",
    "\n",
    "Also, our model treats upper case and lower case forms of the same word differently, but this can be dealt with by preprocessing text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our optimization method and model on an actual text file, which we can download using the NLTK library. We will be using text3, which is the book of Genesis. One important thing to note is that our algorithm doesn't seem to work with punctuation, we will filter those out beforehand. Also note the duration of each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " final_simplex: (array([[0.00e+00, 0.00e+00],\n",
      "       [6.25e-05, 0.00e+00],\n",
      "       [0.00e+00, 6.25e-05]]), array([-0., -0., -0.]))\n",
      "           fun: -0.0\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 11\n",
      "           nit: 3\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([0., 0.])\n",
      "time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "text = list()\n",
    "punc = [',', '.', ':', ';', '?', '!', '\\'', '(', ')', ',)', '.)', ';)', '?)']\n",
    "for word in text:\n",
    "    if (word not in punc):\n",
    "        text.append(str(word))\n",
    "        \n",
    "text = text[:1000]\n",
    "bigramset = list(nltk.bigrams(text[:1000]))\n",
    "setx = set(text[:1000])\n",
    "sety = set(text[:1000])\n",
    "features = [('God', 'created'), ('dry', 'land')]\n",
    "lambdas = [0, 0]\n",
    "result = scipy.optimize.minimize(negativedual, lambdas, method='nelder-mead')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-8deb72807b9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'?'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'!'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'('\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m')'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m',)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m';)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'?)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text3' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "text = list()\n",
    "punc = [',', '.', ':', ';', '?', '!', '\\'', '(', ')', ',)', '.)', ';)', '?)']\n",
    "for word in text3:\n",
    "    if (word not in punc):\n",
    "        text.append(str(word))\n",
    "        \n",
    "text = text[:2000]\n",
    "bigramset = list(nltk.bigrams(text[:2000]))\n",
    "setx = set(text[:2000])\n",
    "sety = set(text[:2000])\n",
    "features = [('God', 'created'), ('dry', 'land')]\n",
    "lambdas = [0, 0]\n",
    "result = scipy.optimize.minimize(negativedual, lambdas, method='nelder-mead')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the algorithm took almost 5 mins to finish optimizing when limiting to the first 1000 words, and over 20 mins when increasing it to 2000 words. This is an indication that it might be a $O(n^2)$ operation, or maybe even exponential. I did not include the full length of the text as it is over 40,000 words, and it isn't hard to imagine how long it would take to finish running on a laptop. However, we can use the previous examples as a proof of concept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Gradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular and computationally cheaper optimization method is a gradient method, which iteratively steps in a direction that minimizes the function(usually determined by the gradient). First we set $\\lambda = 0$. Then for each $\\lambda_i$ we increment it by $\\Delta \\lambda_i$ so that $\\lambda_i = \\lambda_i + \\Delta \\lambda_i$ where $\\Delta \\lambda_i$ is the solution to \n",
    "\n",
    "$$ \\sum_{x,y} \\tilde{p}(x) p(y|x) f_i(x,y) \\exp{(\\Delta \\lambda_i \\sum_{i} f_i(x,y))} = \\tilde{p}(f_i)$$\n",
    "\n",
    "Subtracting $\\tilde{p}(f_i)$ from both sides, we can use the fsolve function in scipy to find the root (in this case $\\lambda_i$). We then check for convergence of $\\lambda_i$ and repeat the increment of $\\lambda_i$ if not.\n",
    "\n",
    "While I did not have the time to learn how to use the fsolve algorithm and fully implement this method, below is an idea of how to implement the above function, find its root, and loop this process for all $\\lambda_i$.\n",
    "\n",
    "First we code the the function of $\\Delta \\lambda_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumf(x, y, features):\n",
    "    total = 0\n",
    "    for feat in features:\n",
    "        total += f((x,y), feat)\n",
    "    return(total)\n",
    "\n",
    "# define each individual feature as dfeat before running\n",
    "def iterate(dlambda):\n",
    "    bigramset = list(nltk.bigrams(text))\n",
    "    totalsum = 0\n",
    "    for x in setx:\n",
    "        for y in sety:\n",
    "            totalsum += ptildex(text, x)*ygivenx(x, y, lambdas, features)*np.exp(dlambda*sumf(x, y, features)) - ptildef(text, bigramset, dfeat)\n",
    "    return(totalsum)\n",
    "\n",
    "dfeat = features[0]\n",
    "scipy.optimize.fsolve(iterate, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement a loop to increment each $\\lambda_i$ and checking for convergence. We can solve each iterative step by using a root finding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0, 0]\n",
    "for i in np.arange(0, len(lambdas)):\n",
    "    lambdai = scipy.optimize.fsolve(iterate, lambdas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dual solution to fit the model is the same as our first method. We expect that this optimization method arrives at the same solution as the dual function is concave and therefore only has one global maximum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to implement and fit a maximum entropy model from scratch! Well, we used libraries like numpy for data structure and scipy for the algorithms, but mostly from scratch. We demonstrated that the maximum entropy model strictly adheres to the features we set for it and randomly predicts when the features are not relevant. Our implementation was very crude and as basic as can be, but it's a good starting off point for more complicated models down the line.\n",
    "\n",
    "Things we learned along the way were that quasi-Newton methods didn't seem to work when optimizing our dual, and although it isn't very clear why, we can guess that it has to do with the Hessian approximation not allowing the algorithm to iterate properly. However, a method that did not use gradients like Nelder-Mead managed to optimize the dual and properly fit the model. We also found that the implementation we used was at least an $O(n^2)$ operation, and working with over a 1000 words woudl require a powerful computer or server to complete in a reasonable time frame.\n",
    "\n",
    "There were also aspects of the implementation that could be improved. For example, I could not figure out how to allow multiple function inputs to work with the scipy optimization algorithms, as they only allowed one function input. To work around this,  variables had to take advantage of a global scope, making for messy and hard to track variable values. This could be further investigated and remedied in the future. There were also issues with very large values when the method calculated function values, although this did not seem to cause any problems. \n",
    "\n",
    "Other future work could be to complete and compare the gradient method to our first one. We could also attempt to choose features for our model. The features we used thus far have been only to test that the model fitting is successful, but not necessarily to make meaningful and useful predictions. Choosing key features would entail adding new features to our existing set and comparing the new model to our previous one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
